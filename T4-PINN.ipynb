{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb121972-edfa-42f1-a3ee-0ca4be4099e5",
   "metadata": {},
   "source": [
    "# üìö PINNs: Neural Network as Solver of the PDE Solution\n",
    "\n",
    "## üîç Neural Network as Solver of the PDE Solution\n",
    "\n",
    "We set up a neural network model $u_\\theta(x,t)$ to approximate the solution $u(x,t) $ of the PDE.  \n",
    "\n",
    "The key idea is to **learn $u_\\theta$ by minimizing the residuals of the PDE and the boundary/initial conditions**, *without* needing the explicit analytical solution of $ u(x, t)$.\n",
    "\n",
    "\n",
    "This means our training objective is to find the network parameters $\\theta $ such that:\n",
    "\n",
    "- The PDE residual $ \\mathcal{R}(x,t) $ is minimized at **collocation points**,\n",
    "- The network output respects the **initial condition** \n",
    "- The network output respects the **boundary conditions** \n",
    "\n",
    "\n",
    "To achieve this, we **pass the residual loss functions** we defined earlier:\n",
    "\n",
    "- $ \\mathcal{L}_{\\text{PDE}} $ for the PDE residual,\n",
    "- $ \\mathcal{L}_{\\text{IC}} $ for the initial condition,\n",
    "- $ \\mathcal{L}_{\\text{BC}} $ for the boundary condition,\n",
    "\n",
    "to an optimizer that minimizes their weighted sum (**the deep learning training process**):\n",
    "\n",
    "$ \\min_{\\theta} \\ \\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}} \\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}} \\mathcal{L}_{\\text{BC}}$\n",
    "\n",
    "This defines the optimization problem underlying our Physics-Informed Neural Network (PINN) framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23683deb-5b76-406a-88c6-55645c6be6fc",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30155577-0197-4229-93f8-1185192ab6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mikelunizar/T4-PINNs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3827a-d4bd-4cdb-8e5e-9034db282e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pinn.src.pde_solver import PDESolver\n",
    "from pinn.src.visuals import plot_collocation_setup, visualize_pde_error_domain\n",
    "from pinn.src.residuals import evaluate_residual_on_collocation_points\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(53)\n",
    "pl.seed_everything(53)\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f247f5-338a-463f-bfca-7306511b2fb2",
   "metadata": {},
   "source": [
    "## üåê Define the PDE Domain and Sampling Strategy\n",
    "\n",
    "In this section, we define the **PDE domain** where our Physics-Informed Neural Network (PINN) will operate.  \n",
    "We discretize the space-time domain, sample **collocation points** where the PDE will be enforced, and define **boundary** and **initial conditions**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de32fe3-c196-4f45-a3f1-f48276aa40a9",
   "metadata": {},
   "source": [
    "\n",
    "#### üßÆ Domain $ (x,t) $\n",
    "\n",
    "We construct a uniform grid in the $(x, t)$ domain:  \n",
    "- $x \\in [-1, 1]$  \n",
    "- $t \\in [0, 1]$  \n",
    "- Resolution: `Nx` points in space and `Nt` points in time  \n",
    "\n",
    "This results in a 2D grid of size $ \\text{Nx} \\times \\text{Nt} $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f43afe-090f-4626-b03f-c9aef0f7c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx, Nt = 300, 200\n",
    "# set grid PDE domain at resolution Nx, Nt\n",
    "x = np.linspace(-1, 1, Nx)\n",
    "t = np.linspace(0, 1, Nt)\n",
    "X, T = np.meshgrid(x, t)\n",
    "xt_grid = np.stack([X.ravel(), T.ravel()], axis=-1)  # shape (N, 2) where N = Nx * Nt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729c474-58ee-4d34-be08-21e9834b43b4",
   "metadata": {},
   "source": [
    "#### üîç Collocation Points\n",
    "\n",
    "From the full grid, we randomly sample:  \n",
    "- `Ncp` **collocation points** where the PDE will be enforced  \n",
    "- These are split into training and validation sets (e.g., 80% / 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda83fa-ef31-46f1-a173-3e10235d2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collocation_points_generation(xt_grid, Ncp, val_ratio=0.2):\n",
    "    # Sample Ncp from grid where to evaluate PDE\n",
    "    xt_cp = xt_grid[np.random.choice(len(xt_grid), Ncp, replace=False)]\n",
    "    # Split the data train, val (80-20%)\n",
    "    train_xt_cp, valid_xt_cp, _, _ = train_test_split(xt_cp, np.zeros(len(xt_cp)), test_size=val_ratio, random_state=52)\n",
    "\n",
    "    return train_xt_cp, valid_xt_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4b259-8414-4a38-9777-0108f83ceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate collocation points\n",
    "Ncp = 500 # Number of Collocation points\n",
    "coll_points_train, coll_points_val = collocation_points_generation(xt_grid, Ncp, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae6662-02d1-42b3-8995-d87af8265317",
   "metadata": {},
   "source": [
    "#### üß± Boundary Conditions (BC)\n",
    "\n",
    "We select `Nbc` points along the **spatial boundaries** $x = -1$ and $x = 1$, across different times.  \n",
    "We impose homogeneous Dirichlet boundary conditions:\n",
    "\n",
    "- $u(-1, t) = u(1, t) = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec7e6a-faad-416f-9409-d88b994381e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition_points(Nbc, xt_grid):\n",
    "    # Define Boundary Conditions\n",
    "    xt_bc = xt_grid[np.argwhere((xt_grid[:, 0] == 1) | (xt_grid[:, 0] == -1)).reshape(-1)]  # all points at x=-1 and x=1\n",
    "    xt_bc = xt_bc[np.random.choice(len(xt_bc), Nbc, replace=False)]\n",
    "    u_bc = np.zeros(xt_bc.shape[0]).reshape(-1, 1)  # set bc of u(-1, t) = u(1, t) = 0\n",
    "    \n",
    "    return xt_bc, u_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12600cc3-959c-494b-b612-177b24e5ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate boundary condition points (x,t) and solution u(x,t)\n",
    "Nbc = 100  # Number of Boundary condition points\n",
    "xt_bc, u_bc = boundary_condition_points(Nbc, xt_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a9206-fb18-4c0a-9048-fabcb1c8f95a",
   "metadata": {},
   "source": [
    "#### üß® Initial Conditions (IC)\n",
    "\n",
    "We select `Nic` points at the **initial time** $t = 0$, across the entire spatial domain.  \n",
    "The initial condition is given by:\n",
    "\n",
    "- $u(x, 0) = -\\sin(\\pi x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad89275-e40a-454e-bdb1-2ed1ac8eea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition_points(Nic, xt_grid):\n",
    "    # Define Initial  Conditions\n",
    "    xt_ic = xt_grid[np.argwhere(xt_grid[:, 1] == 0).reshape(-1)]  # All points with t=0\n",
    "    xt_ic = xt_ic[np.random.choice(len(xt_ic), Nic, replace=False)]\n",
    "    u_ic = -np.sin(np.pi*xt_ic[:, 0]).reshape(-1, 1)  # set ic u(x, 0) = - sin(pi*x) \n",
    "\n",
    "    return xt_ic, u_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007444af-ad67-4ada-b8a4-842a7e0f9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate initial condition points (x,t) and solution u(x,t)\n",
    "Nic = 100  # Number Initial condition points\n",
    "xt_ic, u_ic = initial_condition_points(Nic, xt_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e18f5e-0ee8-46e4-81d0-06b2682b8613",
   "metadata": {},
   "source": [
    "#### üìä Visualization\n",
    "\n",
    "The figure below shows:  \n",
    "- The full space-time  \n",
    "- Collocation points (training and validation)  \n",
    "- Boundary condition points (colored by $u$)  \n",
    "- Initial condition points (colored by $u$)\n",
    "\n",
    "This setup forms the foundation for training the PINN to solve the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2855a8-263d-4bef-b6ba-21af9dd6532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collocation_setup(xt_grid, coll_points_train, coll_points_val, xt_bc, u_bc, xt_ic, u_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bdb4a6-b3c5-4d02-b5a9-3dc71b23e24c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üß© PINN Loss Functions: Residuals and Conditions\n",
    "\n",
    "Our goal is to train a neural network $ u(x,t) $ that satisfies the PDE:\n",
    "\n",
    "##### $ u_t + u \\cdot u_x - \\nu u_{xx} = 0 $\n",
    "\n",
    "where:\n",
    "- $ u_t = \\frac{\\partial u}{\\partial t} $ (time derivative)  \n",
    "- $ u_x = \\frac{\\partial u}{\\partial x} $ (spatial derivative)  \n",
    "- $ u_{xx} = \\frac{\\partial^2 u}{\\partial x^2} $ (second spatial derivative)  \n",
    "- $ \\nu=0.025 $ is the viscosity coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b1fef-772b-4daf-86db-19fd58723c4e",
   "metadata": {},
   "source": [
    "\n",
    "#### PDE Residual Loss\n",
    "\n",
    "At **collocation points** $ (x,t) $, the PDE residual should be zero:\n",
    "\n",
    "$ \\mathcal{R}(x,t) = u_t + u u_x - \\nu u_{xx} \\approx 0 $\n",
    "\n",
    "The PDE loss minimizes the mean squared residual:\n",
    "\n",
    "$ \\mathcal{L}_{\\text{PDE}} = \\frac{1}{N_{cp}} \\sum_{i=1}^{N_{cp}} \\mathcal{R}(x_i, t_i)^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e626f-b3c7-4b86-a9ea-9f50764035a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physical parameters\n",
    "nu = 0.025 \n",
    "\n",
    "def residual_pde(xt_cp, model):\n",
    "    \"\"\"\n",
    "    Compute PDE residual loss at collocation points:\n",
    "    \"\"\"\n",
    "    x = xt_cp[:, 0:1].clone().detach().requires_grad_(True)  # activate gradients\n",
    "    t = xt_cp[:, 1:].clone().detach().requires_grad_(True)   # activate gradients\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        inputs = torch.cat([x, t], dim=1)\n",
    "        u_hat = model(inputs)\n",
    "        \n",
    "        # First derivatives\n",
    "        du_dt = torch.autograd.grad(u_hat, t, grad_outputs=torch.ones_like(u_hat), create_graph=True)[0]\n",
    "        du_dx = torch.autograd.grad(u_hat, x, grad_outputs=torch.ones_like(u_hat), create_graph=True)[0]\n",
    "        \n",
    "        # Second derivative wrt x\n",
    "        d2u_dx2 = torch.autograd.grad(du_dx, x, grad_outputs=torch.ones_like(du_dx), create_graph=True)[0]\n",
    "\n",
    "        # PDE residual: u_t + u * u_x - nu * u_xx = 0\n",
    "        residual = du_dt + u_hat * du_dx - nu * d2u_dx2\n",
    "        \n",
    "        residual_pde_ = torch.mean(residual ** 2)\n",
    "        \n",
    "    return residual_pde_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182889f9-29e2-459d-8ac9-caf61054fc9a",
   "metadata": {},
   "source": [
    "#### Initial Condition Residual Loss\n",
    "\n",
    "At initial time $ t=0 $, the network output must match the initial condition:\n",
    "\n",
    "$ u(x,0) = u_{\\text{ic}}(x) $\n",
    "\n",
    "The initial condition loss works as a data-driven approach, because the solution $u(x,t)$ is known:\n",
    "\n",
    "$ \\mathcal{L}_{\\text{IC}} = \\frac{1}{N_{ic}} \\sum_{i=1}^{N_{ic}} \\left( u(x_i,0) - u_{\\text{ic}}(x_i) \\right)^2 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b539ec8-b17c-4629-bb90-4d5feb7f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_ic(xt_ic, u_ic, model):\n",
    "    \"\"\"\n",
    "    Compute MSE loss for Initial Condition:\n",
    "    \"\"\"\n",
    "    u_pred = model(xt_ic)\n",
    "    return torch.mean((u_ic - u_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688d3c9-e35a-4660-be39-fb6e226e52a6",
   "metadata": {},
   "source": [
    "#### Boundary Condition Residual Loss\n",
    "\n",
    "On spatial boundaries $ x = \\pm 1 $, enforce:\n",
    "\n",
    "$ u(-1, t) = u(1, t) = 0 $\n",
    "\n",
    "The boundary loss is:\n",
    "\n",
    "$ \\mathcal{L}_{\\text{BC}} = \\frac{1}{N_{bc}} \\sum_{i=1}^{N_{bc}} \\left( u(x_i, t_i) - u_{\\text{bc}}(x_i, t_i) \\right)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b716a-11c4-462a-8c30-625c17a927b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_bc(xt_bc, u_bc, model):\n",
    "    \"\"\"\n",
    "    Compute MSE loss for Boundary Condition:\n",
    "    \"\"\"\n",
    "    u_pred = model(xt_bc)\n",
    "    return torch.mean((u_pred - u_bc) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf4e2cc-9c7e-4bf9-afe9-cb1d11d39712",
   "metadata": {},
   "source": [
    "#### Total Residual to minimize\n",
    "\n",
    "The total loss is typically a weighted sum:\n",
    "\n",
    "$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{PDE}} + \\lambda_{\\text{IC}} \\mathcal{L}_{\\text{IC}} + \\lambda_{\\text{BC}} \\mathcal{L}_{\\text{BC}}\n",
    "$\n",
    "\n",
    "which is minimized during training to solve the PDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6439d0a-f6ee-4b58-81ea-b9ebd11e0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ic, lambda_bc = 1, 1  # set weights\n",
    "\n",
    "def residual_loss(xt_cp, xt_bc, u_bc, xt_ic, u_ic, model):\n",
    "    # PDE residual at collocation points\n",
    "    residual_pde_cost = residual_pde(xt_cp, model)\n",
    "    # Initial Conditions residual at i.c. points\n",
    "    residual_ic_cost = residual_ic(xt_ic, u_ic, model)\n",
    "    # Boundary Condition residual at b.c. points\n",
    "    residual_bc_cost = residual_bc(xt_bc, u_bc, model)\n",
    "\n",
    "    # Total residual: Weighted sum\n",
    "    residual = residual_pde_cost + lambda_ic * residual_ic_cost +  lambda_bc * residual_bc_cost\n",
    "\n",
    "    return {'residual': residual, 'residual_pde': residual_pde_cost, 'residual_bc': residual_bc_cost, 'residual_ic': residual_ic_cost} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855e0c4-7f1a-4c67-b889-41b79c309695",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "The model is a fully connected feedforward neural network defined as:\n",
    "\n",
    "$\n",
    "\\text{Input} \\rightarrow \\text{Linear} \\rightarrow \\text{Activation} \\rightarrow \\text{Linear} \\rightarrow \\text{Activation}\\rightarrow \\text{Linear} \\rightarrow (Output)\n",
    "$\n",
    "\n",
    "- **Input dimension**: 2 (raw $(x,t)$ or 22 if Fourier feature encoding is used.\n",
    "- **Hidden layers**: 3 layers, each with 64 neurons.\n",
    "- **Activation**: Hyperbolic tangent $\\tanh$ applied after each hidden layer.\n",
    "\n",
    "\n",
    "#### Choose the Activation Function\n",
    "\n",
    "Activation functions affect how well the network learns and how smooth the predicted solution is.\n",
    "\n",
    "**Think about:**  \n",
    "- What happens if the activation is not smooth or not differentiable everywhere?  \n",
    "- How might that impact the gradients needed for training a PINN?  \n",
    "- Why could having multiple continuous derivatives be important for solving PDEs?\n",
    "\n",
    "Choose wisely and test how it influences your results!\n",
    "\n",
    "| Activation | Continuity       | Differentiability              | Non-zero Derivatives                    |\n",
    "|------------|------------------|-------------------------------|------------------------------------|\n",
    "| **ReLU**   | Continuous       | Not differentiable at 0        | 1st derivative non-zero except at 0 | \n",
    "| **Tanh**   | Continuous       | Infinitely differentiable       | All derivatives are non-zero        |\n",
    "| **Sigmoid**| Continuous       | Infinitely differentiable       | All derivatives are non-zero        | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae7c26f-0ef1-4d55-ac29-34552e66cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, output and hidden size\n",
    "input_size = 2\n",
    "ouput_size = 1\n",
    "hiden_size = 64 \n",
    "\n",
    "# NN model\n",
    "model = torch.nn.Sequential(torch.nn.Linear(input_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, ouput_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa2f97-e81a-4232-9f73-31fe8fc7893c",
   "metadata": {},
   "source": [
    "## Solving the PDE with PINNs: Training the Model\n",
    "\n",
    "Now that you have designed your neural network and chosen the activation functions, it‚Äôs time to **train the model** to solve the PDE using a Physics-Informed Neural Network (PINN).\n",
    "\n",
    "During training, the network learns to minimize the PDE residual while satisfying the boundary and initial conditions by adjusting its parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06effd4a-5d6c-40a9-9ea0-afbc90615c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# hyperparameters\n",
    "opt = torch.optim.Adam\n",
    "lr = 1e-3\n",
    "bs = 16\n",
    "epochs = 100\n",
    "foldername = f'bs={bs}_lr={lr}_Ncp={Ncp}' # Run name\n",
    "\n",
    "# Solver setup\n",
    "pde_solver = PDESolver(model, residual=residual_loss,\n",
    "               xt_bc=xt_bc, u_bc=u_bc, xt_ic=xt_ic, u_ic=u_ic,\n",
    "               lr=lr, optimizer=opt, device=device)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='valid_residual', patience=25, mode='min') # regularization technique\n",
    "logger = WandbLogger(name=foldername, project='T4-PINNs') # W&B logger\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "                     max_epochs=epochs, \n",
    "                     logger=logger,callbacks=[early_stopping],\n",
    "                     check_val_every_n_epoch=10)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(torch.tensor(coll_points_train, dtype=torch.float32), batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(torch.tensor(coll_points_val, dtype=torch.float32), batch_size=32, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(pde_solver, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1ffc8-51ff-4ec8-b196-04b4ac5ae1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inference: Provide Dense Solution\n",
    "\n",
    "One of the coolest advantages of PINNs is that, once trained, the neural network acts as a **continuous function approximator** over the whole domain.\n",
    "\n",
    "This means we can perform **inference** to predict the solution at **any point** inside the domain‚Äînot just at training points or a fixed grid.\n",
    "\n",
    "**PINNs**:\n",
    "\n",
    "- We get a **dense, smooth solution** without needing to solve the PDE again on a fine mesh.\n",
    "- This flexibility allows us to:\n",
    "  - Visualize the solution in high resolution,\n",
    "  - Evaluate the solution at points of interest,\n",
    "  - Use the model for further analysis or control tasks.\n",
    "  \n",
    "In essence, the PINN provides a **mesh-free, continuous representation** of the PDE solution, making it incredibly versatile compared to classical numerical methods.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2224ba2-9419-4f64-82fb-1ef8eaa43f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform prediction\n",
    "u_grid_hat = model(torch.tensor(xt_grid, dtype=torch.float32)).detach()\n",
    "\n",
    "# Create a figure with 1 row and 2 columns of subplots\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "# Left subplot - prediction\n",
    "sc1 = ax1.scatter(xt_grid[:, 0:1], xt_grid[:, 1:], c=u_grid_hat, cmap='jet')\n",
    "ax1.set_title(f'Resolution {Nx}x{Nt}')\n",
    "fig.colorbar(sc1, ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba354d0-87f4-4968-941d-bbe5b50de35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High resolution Solution\n",
    "Nx_high_res, Nt_high_res = 1000, 1000\n",
    "# set grid PDE domain at resolution Nx, Nt\n",
    "x_high_res = np.linspace(-1, 1, Nx_high_res)\n",
    "t_high_res = np.linspace(0, 1, Nt_high_res)\n",
    "X_high_res, T_high_res = np.meshgrid(x_high_res, t_high_res)\n",
    "xt_high_res = np.stack([X_high_res.ravel(), T_high_res.ravel()], axis=-1)  # shape (N, 2) where N = Nx * Nt\n",
    "# perform prediction\n",
    "u_grid_hihg_res_hat = model(torch.tensor(xt_high_res, dtype=torch.float32)).detach()\n",
    "\n",
    "# Create a figure with 1 row and 2 columns of subplots\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "# Left subplot - prediction\n",
    "sc1 = ax1.scatter(xt_high_res[:, 0:1], xt_high_res[:, 1:], c=u_grid_hihg_res_hat, cmap='jet')\n",
    "ax1.set_title(f'Resolution {Nx_high_res}x{Nt_high_res}')\n",
    "fig.colorbar(sc1, ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446a0e6-28fb-4de5-8e8e-72f7b86f9509",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PDE Residual over Domain\n",
    "\n",
    "When solving PDEs with Physics-Informed Neural Networks (PINNs), **plotting the PDE residual over the domain is crucial**.\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- The **residual** measures how well the network's predicted solution satisfies the underlying PDE at each point in the domain.\n",
    "- Since PINNs incorporate the PDE directly into the loss function, a low residual means the solution closely follows the physics.\n",
    "- Importantly, the residual can be computed **even where we don't have the exact or reference solution available**.\n",
    "\n",
    "This makes the PDE residual a powerful, model-based **error indicator**:\n",
    "\n",
    "- It allows us to **estimate the local error** of the solution without requiring ground truth data.\n",
    "- We can identify regions in the domain where the model struggles, guiding refinement strategies such as:\n",
    "  - Adding more collocation points,\n",
    "  - Increasing network capacity,\n",
    "  - Adjusting training parameters.\n",
    "\n",
    "**In summary:**  \n",
    "Visualizing the residual helps us **trust** and **diagnose** the PINN solution beyond known points, giving insight into the quality and reliability of the PDE solver in unexplored areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f793a4d-30c4-458f-b541-59a865cf6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the residuals\n",
    "residuals = evaluate_residual_on_collocation_points(xt_grid, model, residual_pde_fn=residual_pde)\n",
    "residuals = residuals.reshape(T.shape)  # if xt_grid was created from meshgrid (X, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52981723-e375-40ab-a3ff-1770501bed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the residual errors\n",
    "visualize_pde_error_domain(residuals, X, T, coll_points_train, coll_points_val, log=False)\n",
    "visualize_pde_error_domain(residuals, X, T, coll_points_train, coll_points_val, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406f5ee-6e71-4955-ae78-9321c65d4012",
   "metadata": {},
   "source": [
    "### PINN Solution vs. Finite Differences\n",
    "\n",
    "After obtaining the PINN solution, it is important to compare it with a high-quality (HQ) reference solution computed using classical numerical methods, such as Finite Differences (FD).\n",
    "\n",
    "**Key point:**  \n",
    "The PINN was trained **only using a relatively small set of collocation points**, often far fewer than traditional numerical methods require.  \n",
    "In contrast, the Finite Differences solution is computed on a **fine mesh with over one million nodes ($> 10^6$)**, providing a very detailed and accurate reference.\n",
    "\n",
    "This comparison showcases how PINNs can approximate complex PDE solutions efficiently, using far fewer data points than classical mesh-based methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f6a1a-6f2f-479f-82a7-34b67bbb22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_finite_differences_solution():\n",
    "    # Target resolution\n",
    "    x_target = np.linspace(-1, 1, Nx)\n",
    "    t_target = np.linspace(0, 1, Nt)\n",
    "    \n",
    "    # Fine resolution for stability\n",
    "    Nx_fine = 1024\n",
    "    x_fine = np.linspace(-1, 1, Nx_fine)\n",
    "    dx = x_fine[1] - x_fine[0]\n",
    "    \n",
    "    dt = 0.00001\n",
    "    Nt_fine = int(1.0 / dt)\n",
    "    save_times = np.linspace(0, Nt_fine-1, Nt, dtype=int)  # time indices to save\n",
    "    \n",
    "    # Initialize fine grid\n",
    "    u = -np.sin(np.pi * x_fine)\n",
    "    u[0] = 0\n",
    "    u[-1] = 0\n",
    "    \n",
    "    # Output array (Nt, Nx)\n",
    "    u_all = np.zeros((Nt, Nx))\n",
    "    \n",
    "    # Save initial state (interpolated)\n",
    "    u_all[0] = np.interp(x_target, x_fine, u)\n",
    "    \n",
    "    frame = 1\n",
    "    for n in range(1, Nt_fine):\n",
    "        un = u.copy()\n",
    "        u[1:-1] = (un[1:-1]\n",
    "                   - dt * un[1:-1] * (un[2:] - un[:-2]) / (2 * dx)\n",
    "                   + nu * dt * (un[2:] - 2 * un[1:-1] + un[:-2]) / dx**2)\n",
    "        u[0] = 0\n",
    "        u[-1] = 0\n",
    "    \n",
    "        if n == save_times[frame]:\n",
    "            u_all[frame] = np.interp(x_target, x_fine, u)\n",
    "            frame += 1\n",
    "            if frame == Nt:\n",
    "                break\n",
    "                \n",
    "    return u_all\n",
    "\n",
    "\n",
    "def plot_pinn_vs_finite_diff_results(xt_grid, u_grid_hat, u_solution):\n",
    "    \"\"\"\n",
    "    Plot predicted solution, target solution, and absolute error for PINNs.\n",
    "\n",
    "    Parameters:\n",
    "    - xt_grid: Tensor or ndarray of shape (N, 2) with (x,t) coordinates.\n",
    "    - u_grid_hat: Tensor or ndarray of predicted solution values of shape (N,).\n",
    "    - u_solution: ndarray or tensor of target solution values (can be multidim, will be flattened).\n",
    "    \"\"\"\n",
    "    u_target = torch.tensor(u_solution.flatten(), dtype=torch.float32)\n",
    "    error = torch.abs(u_grid_hat.flatten() - u_target)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 6))\n",
    "\n",
    "    # Prediction subplot\n",
    "    sc1 = axs[0].scatter(xt_grid[:, 0], xt_grid[:, 1], c=u_grid_hat.flatten(), cmap='jet', s=5)\n",
    "    axs[0].set_title('Predicted Solution')\n",
    "    fig.colorbar(sc1, ax=axs[0])\n",
    "\n",
    "    # Target subplot\n",
    "    sc2 = axs[1].scatter(xt_grid[:, 0], xt_grid[:, 1], c=u_target, cmap='jet', s=5)\n",
    "    axs[1].set_title('Target Solution')\n",
    "    fig.colorbar(sc2, ax=axs[1])\n",
    "\n",
    "    # Error subplot\n",
    "    sc3 = axs[2].scatter(xt_grid[:, 0], xt_grid[:, 1], c=error, cmap='inferno', s=5)\n",
    "    axs[2].set_title('Absolute Error')\n",
    "    fig.colorbar(sc3, ax=axs[2])\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$t$')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8753b8e-d55f-4740-91e2-8019235ebde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Finite Differences Solution\n",
    "u_solution = generate_finite_differences_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810835b9-9541-4735-affa-9505c541c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare solution PINN vs Finite Differences\n",
    "plot_pinn_vs_finite_diff_results(xt_grid, u_grid_hat, u_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204c1ab-4240-404e-99fd-9d8b8f7ef83f",
   "metadata": {},
   "source": [
    "# üöÄ Do It Yourself: Explore New Physics!\n",
    "\n",
    "Now it‚Äôs your turn!  \n",
    "Let‚Äôs explore how changing the **Initial Conditions (IC)** leads to entirely new solutions of the same PDE.\n",
    "\n",
    "üß† **Remember:**  \n",
    "For every combination of **PDE**, **Initial Conditions (IC)**, and **Boundary Conditions (BC)**, there's one **unique solution** (under well-posed conditions).  \n",
    "That means even small changes to the initial setup can drastically affect the behavior of the solution!\n",
    "\n",
    "---\n",
    "\n",
    "### üß® Try New Initial Conditions: New Physics, New Patterns!\n",
    "\n",
    "We define the initial condition at **time $t = 0$** by selecting `Nic` points across the spatial domain.  \n",
    "This is your opportunity to see how the system evolves from different starting states.\n",
    "\n",
    "Here are some examples to try:\n",
    "\n",
    "- $u(x, 0) = -\\sin(\\pi x)$\n",
    "- $u(x, 0) = -\\sin(2\\pi x)$\n",
    "- $u(x, 0) = -\\sin(4\\pi x)$\n",
    "- $u(x, 0) = \\text{your custom function!} \\; üé®$\n",
    "\n",
    "---\n",
    "\n",
    "Go ahead, tweak the IC, retrain the PINN, and **discover how the physics responds**.  \n",
    "This is where PINNs shine: solving new problems with just a few modifications and no re-meshing!\n",
    "\n",
    "üéØ **Challenge:** Try a custom IC and compare the result to a known FD solution.  \n",
    "Can you get a good match? Where does it fail?\n",
    "\n",
    "---\n",
    "\n",
    "> ‚ú® Tip: Use the residual and error plots to evaluate how your new IC affects training and accuracy.\n",
    "\n",
    "### üéöÔ∏è Tune the Physics: Adjust Loss Weights\n",
    "\n",
    "Want more control over how your model learns the physics?  \n",
    "You can **tweak the weights** of the residual loss components to influence how strongly the model enforces:\n",
    "\n",
    "- ‚öì **Boundary Conditions (BC)**\n",
    "- üïí **Initial Conditions (IC)**\n",
    "- üßÆ **PDE Residual**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d21bd8-290a-4aa7-8cdf-2487a43171f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition_points(Nic, xt_grid):\n",
    "    # Define Initial  Conditions\n",
    "    xt_ic = xt_grid[np.argwhere(xt_grid[:, 1] == 0).reshape(-1)]  # All points with t=0\n",
    "    xt_ic = xt_ic[np.random.choice(len(xt_ic), Nic, replace=False)]\n",
    "    \n",
    "    # here is where you define the initial conditions\n",
    "    u_ic = #TODO  -np.sin(4*np.pi*xt_ic[:, 0]).reshape(-1, 1)  # set ic u(x, 0) = - sin(pi*x) \n",
    "\n",
    "    return xt_ic, u_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dd8cea-a309-4104-b43e-b6619a7b3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ic, lambda_bc = #TODO set weights residuals\n",
    "\n",
    "def residual_loss(xt_cp, xt_bc, u_bc, xt_ic, u_ic, model):\n",
    "    # PDE residual at collocation points\n",
    "    residual_pde_cost = residual_pde(xt_cp, model)\n",
    "    # Initial Conditions residual at i.c. points\n",
    "    residual_ic_cost = residual_ic(xt_ic, u_ic, model)\n",
    "    # Boundary Condition residual at b.c. points\n",
    "    residual_bc_cost = residual_bc(xt_bc, u_bc, model)\n",
    "\n",
    "    # Total residual: Weighted sum\n",
    "    residual = residual_pde_cost + lambda_ic * residual_ic_cost +  lambda_bc * residual_bc_cost\n",
    "\n",
    "    return {'residual': residual, 'residual_pde': residual_pde_cost, 'residual_bc': residual_bc_cost, 'residual_ic': residual_ic_cost} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ca461-8806-47d1-a445-6ec4c8603a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the collocation points\n",
    "Ncp = #TODO 5000\n",
    "Nic = #TODO 300\n",
    "Nbc = #TODO 300\n",
    "\n",
    "# Generate collocation points\n",
    "coll_points_train, coll_points_val = collocation_points_generation(xt_grid, Ncp, val_ratio=0.2)\n",
    "\n",
    "# Generate initial condition points (x,t) and solution u(x,t)\n",
    "xt_ic, u_ic = initial_condition_points(Nic, xt_grid)\n",
    "\n",
    "# Generate initial condition points (x,t) and solution u(x,t)\n",
    "xt_bc, u_bc = boundary_condition_points(Nbc, xt_grid)\n",
    "\n",
    "# Visualize your new problem\n",
    "plot_collocation_setup(xt_grid, coll_points_train, coll_points_val, xt_bc, u_bc, xt_ic, u_ic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0a882-ef93-44cd-9f74-13007326d797",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Architecture & Solve Your New PDE\n",
    "\n",
    "Now that you've chosen a new **Initial Condition**, it's time to build your own PINN model to solve the updated PDE problem!\n",
    "\n",
    "\n",
    "üîç **Reminder:** Smooth activation functions like `tanh` are often better for PDEs, especially when we need to compute higher-order derivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980d135-1a4f-4833-8c28-816ac1ae80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, output and hidden size\n",
    "input_size = 2\n",
    "ouput_size = 1\n",
    "hiden_size = 128 \n",
    "\n",
    "# NN model\n",
    "model = torch.nn.Sequential(torch.nn.Linear(input_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, hiden_size),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hiden_size, ouput_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de152c-c824-48f5-90b7-f89ac2f40f5f",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è 2. Set Your Optimization Hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505e22b-6a12-4865-a304-ae3e3e00929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "opt = torch.optim.Adam\n",
    "lr = 1e-3\n",
    "bs = 16\n",
    "epochs = 2000\n",
    "foldername = f'bs={bs}_lr={lr}_Ncp={Ncp}' # Run name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8d3af-61b7-4787-b147-8e475a2fcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solver setup\n",
    "pde_solver = PDESolver(model, residual=residual_loss,\n",
    "               xt_bc=xt_bc, u_bc=u_bc, xt_ic=xt_ic, u_ic=u_ic,\n",
    "               lr=lr, optimizer=opt, device=device)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = pl.callbacks.EarlyStopping(monitor='valid_residual', patience=25, mode='min') # regularization technique\n",
    "logger = WandbLogger(name=foldername, project='T4-PINNs') # W&B logger\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "                     max_epochs=epochs, \n",
    "                     logger=logger,callbacks=[early_stopping],\n",
    "                     check_val_every_n_epoch=10)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(torch.tensor(coll_points_train, dtype=torch.float32), batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(torch.tensor(coll_points_val, dtype=torch.float32), batch_size=32, shuffle=False)\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(pde_solver, train_loader, valid_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5cd65-fc0a-4f33-9e81-9e90513a7bcc",
   "metadata": {},
   "source": [
    "### Plot PDE High resolution Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a49311-7e3b-4f5b-ae05-1687865b8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High resolution Solution\n",
    "Nx_high_res, Nt_high_res = 500, 500\n",
    "# set grid PDE domain at resolution Nx, Nt\n",
    "x_high_res = np.linspace(-1, 1, Nx_high_res)\n",
    "t_high_res = np.linspace(0, 1, Nt_high_res)\n",
    "X_high_res, T_high_res = np.meshgrid(x_high_res, t_high_res)\n",
    "xt_high_res = np.stack([X_high_res.ravel(), T_high_res.ravel()], axis=-1)  # shape (N, 2) where N = Nx * Nt\n",
    "# perform prediction\n",
    "u_grid_hihg_res_hat = u_pde_model(torch.tensor(xt_high_res, dtype=torch.float32)).detach()\n",
    "\n",
    "# Create a figure with 1 row and 2 columns of subplots\n",
    "fig, (ax1) = plt.subplots(1, 1)\n",
    "# Left subplot - prediction\n",
    "sc1 = ax1.scatter(xt_high_res[:, 0:1], xt_high_res[:, 1:], c=u_grid_hihg_res_hat, cmap='jet')\n",
    "ax1.set_title(f'Resolution {Nx_high_res}x{Nt_high_res}')\n",
    "fig.colorbar(sc1, ax=ax1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
